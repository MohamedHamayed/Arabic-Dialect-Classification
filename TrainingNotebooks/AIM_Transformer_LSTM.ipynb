{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, BertModel, AdamW\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../PreprocessedData/preprocessed_data.csv',sep='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>dialect</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>450623</th>\n",
       "      <td>1164904153652617216</td>\n",
       "      <td>@_anwaralsalim Ø§Ù„Ù„Ù‡Ù… Ø§Ù…ÙŠÙŠÙ† ÙŠØ§Ø±Ø¨ Ø§Ù„Ø¹Ø§Ø§Ù„Ù…ÙŠÙŠÙ† ğŸ™ğŸ¼</td>\n",
       "      <td>BH</td>\n",
       "      <td>[Ù…Ø³ØªØ®Ø¯Ù…] Ø§Ù„Ù„Ù‡Ù… Ø§Ù…ÙŠÙŠÙ† ÙŠØ§Ø±Ø¨ Ø§Ù„Ø¹Ø§Ø§Ù„Ù…ÙŠÙŠÙ†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6726</th>\n",
       "      <td>1126203932722847616</td>\n",
       "      <td>@EdyCohen ÙˆÙ„Ø§ ÙˆØ§Ø­Ø¯ ÙƒÙ„Ù‡Ù… Ù†ÙƒØ±Ø§Øª</td>\n",
       "      <td>IQ</td>\n",
       "      <td>[Ù…Ø³ØªØ®Ø¯Ù…] ÙˆÙ„Ø§ ÙˆØ§Ø­Ø¯ ÙƒÙ„Ù‡Ù… Ù†ÙƒØ±Ø§Øª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93275</th>\n",
       "      <td>1135183573613449216</td>\n",
       "      <td>ÙƒÙ†Øª Ø§Ù†Ø§ Ùˆ @EsraaRaie  Ø¨Ø§Ù„ØªØ§ÙƒØ³ÙŠ Ø§Ù„Ø³ÙˆØ§Ù‚ Ø¨Ù‚ÙˆÙ„Ù†Ø§ Ø§...</td>\n",
       "      <td>PL</td>\n",
       "      <td>ÙƒÙ†Øª Ø§Ù†Ø§ Ùˆ [Ù…Ø³ØªØ®Ø¯Ù…] Ø¨Ø§Ù„ØªØ§ÙƒØ³ÙŠ Ø§Ù„Ø³ÙˆØ§Ù‚ Ø¨Ù‚ÙˆÙ„Ù†Ø§ Ø§Ø®Ø¯Øª...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409451</th>\n",
       "      <td>537560284928307200</td>\n",
       "      <td>#..\\n\\nîš‰îš®#__ Ø¢ÙŠ Ø¹Ù„Ø¢Ù‚Ø© Ù…Ù† Ø¢Ù„Ø¹Ø¢Ù„Ù… Ø¢Ù„Ø¢ÙØªØ±Ø¢Ø¶ÙŠ Ø¹Ù„Ø¢Ù‚...</td>\n",
       "      <td>AE</td>\n",
       "      <td># . . # _ _ Ø¢ÙŠ Ø¹Ù„Ø¢Ù‚Ø© Ù…Ù† Ø¢Ù„Ø¹Ø¢Ù„Ù… Ø¢Ù„Ø¢ÙØªØ±Ø¢Ø¶ÙŠ Ø¹Ù„Ø¢Ù‚Ø©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270687</th>\n",
       "      <td>939985298506829824</td>\n",
       "      <td>Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ Ø¹Ù„Ù‰ Ù†Ø¹Ù…Ù‡ Ø§Ù„ØªØ±Ø¨ÙŠÙ‡ Ø§Ù„ÙƒÙˆÙŠØ³Ù‡ ÙˆØ§Ù„Ø¹ÙŠÙ† Ø§Ù„Ù…Ù„...</td>\n",
       "      <td>EG</td>\n",
       "      <td>Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ Ø¹Ù„Ù‰ Ù†Ø¹Ù…Ù‡ Ø§Ù„ØªØ±Ø¨ÙŠÙ‡ Ø§Ù„ÙƒÙˆÙŠØ³Ù‡ ÙˆØ§Ù„Ø¹ÙŠÙ† Ø§Ù„Ù…Ù„...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "450623  1164904153652617216   \n",
       "6726    1126203932722847616   \n",
       "93275   1135183573613449216   \n",
       "409451   537560284928307200   \n",
       "270687   939985298506829824   \n",
       "\n",
       "                                                     text dialect  \\\n",
       "450623      @_anwaralsalim Ø§Ù„Ù„Ù‡Ù… Ø§Ù…ÙŠÙŠÙ† ÙŠØ§Ø±Ø¨ Ø§Ù„Ø¹Ø§Ø§Ù„Ù…ÙŠÙŠÙ† ğŸ™ğŸ¼      BH   \n",
       "6726                        @EdyCohen ÙˆÙ„Ø§ ÙˆØ§Ø­Ø¯ ÙƒÙ„Ù‡Ù… Ù†ÙƒØ±Ø§Øª      IQ   \n",
       "93275   ÙƒÙ†Øª Ø§Ù†Ø§ Ùˆ @EsraaRaie  Ø¨Ø§Ù„ØªØ§ÙƒØ³ÙŠ Ø§Ù„Ø³ÙˆØ§Ù‚ Ø¨Ù‚ÙˆÙ„Ù†Ø§ Ø§...      PL   \n",
       "409451  #..\\n\\nîš‰îš®#__ Ø¢ÙŠ Ø¹Ù„Ø¢Ù‚Ø© Ù…Ù† Ø¢Ù„Ø¹Ø¢Ù„Ù… Ø¢Ù„Ø¢ÙØªØ±Ø¢Ø¶ÙŠ Ø¹Ù„Ø¢Ù‚...      AE   \n",
       "270687  Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ Ø¹Ù„Ù‰ Ù†Ø¹Ù…Ù‡ Ø§Ù„ØªØ±Ø¨ÙŠÙ‡ Ø§Ù„ÙƒÙˆÙŠØ³Ù‡ ÙˆØ§Ù„Ø¹ÙŠÙ† Ø§Ù„Ù…Ù„...      EG   \n",
       "\n",
       "                                        preprocessed_text  \n",
       "450623               [Ù…Ø³ØªØ®Ø¯Ù…] Ø§Ù„Ù„Ù‡Ù… Ø§Ù…ÙŠÙŠÙ† ÙŠØ§Ø±Ø¨ Ø§Ù„Ø¹Ø§Ø§Ù„Ù…ÙŠÙŠÙ†  \n",
       "6726                         [Ù…Ø³ØªØ®Ø¯Ù…] ÙˆÙ„Ø§ ÙˆØ§Ø­Ø¯ ÙƒÙ„Ù‡Ù… Ù†ÙƒØ±Ø§Øª  \n",
       "93275   ÙƒÙ†Øª Ø§Ù†Ø§ Ùˆ [Ù…Ø³ØªØ®Ø¯Ù…] Ø¨Ø§Ù„ØªØ§ÙƒØ³ÙŠ Ø§Ù„Ø³ÙˆØ§Ù‚ Ø¨Ù‚ÙˆÙ„Ù†Ø§ Ø§Ø®Ø¯Øª...  \n",
       "409451  # . . # _ _ Ø¢ÙŠ Ø¹Ù„Ø¢Ù‚Ø© Ù…Ù† Ø¢Ù„Ø¹Ø¢Ù„Ù… Ø¢Ù„Ø¢ÙØªØ±Ø¢Ø¶ÙŠ Ø¹Ù„Ø¢Ù‚Ø©...  \n",
       "270687  Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ Ø¹Ù„Ù‰ Ù†Ø¹Ù…Ù‡ Ø§Ù„ØªØ±Ø¨ÙŠÙ‡ Ø§Ù„ÙƒÙˆÙŠØ³Ù‡ ÙˆØ§Ù„Ø¹ÙŠÙ† Ø§Ù„Ù…Ù„...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['preprocessed_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"aubmindlab/bert-base-arabertv02-twitter\"\n",
    "arabert_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_list, classes_list, tokenizer):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        self.text_list = text_list\n",
    "        self.classes_list = classes_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = 0\n",
    "        self.create_dataset()\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        input_ids = self.features[index]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_length = self.max_len - len(input_ids)\n",
    "        input_ids = ([self.tokenizer.pad_token_id] * padding_length) + input_ids \n",
    "        attention_mask = ([0] * padding_length) + attention_mask  \n",
    "\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        label = torch.tensor(int(self.labels[index]))\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def create_dataset(self):\n",
    "        lines = []\n",
    "  \n",
    "        for line in self.text_list:\n",
    "            line_tokenized = self.tokenizer.encode(line)\n",
    "            self.max_len = max(self.max_len,len(line_tokenized))\n",
    "            lines.append(line_tokenized)\n",
    "\n",
    "        self.features = lines\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        self.labels = le.fit_transform(self.classes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = bert_dataset(df['preprocessed_text'], df['dialect'], arabert_tokenizer)\n",
    "\n",
    "dataset_len = len(dataset)\n",
    "\n",
    "train_len = int(len(dataset)*0.9)\n",
    "test_len = dataset_len - train_len\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_len, test_len], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=8, shuffle=True)\n",
    "\n",
    "dataloaders_dict = {'train':train_loader, 'val':test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02-twitter were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02-twitter and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class BERT(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(BERT, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(model_name)\n",
    "        self.lstm = torch.nn.LSTM(768, 128, batch_first=True)\n",
    "        self.out = torch.nn.Linear(128, 18)\n",
    "        \n",
    "    def forward(self,ids,attention_mask):\n",
    "        outputs = self.bert_model(ids,attention_mask= attention_mask, output_hidden_states=True)\n",
    "        o2 = outputs[2][-1]\n",
    "        lstm_out, (ht, ct) = self.lstm(o2)\n",
    "        out= self.out(ht[-1])\n",
    "        \n",
    "        return out\n",
    "    \n",
    "model = BERT(model_name)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "#     print(name)\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "# print(len(params_to_update))\n",
    "\n",
    "#Initialize Optimizer\n",
    "optimizer = AdamW(params_to_update, lr=0.001, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lstm): LSTM(768, 128, batch_first=True)\n",
       "  (out): Linear(in_features=128, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=5):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    num_training_steps = num_epochs*dataset_len//32\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for input_ids, input_mask, labels in dataloaders[phase]:\n",
    "                \n",
    "                input_ids = input_ids.to(device)\n",
    "                input_mask  = input_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(input_ids,attention_mask=input_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * input_ids.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, hist = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=6)\n",
    "torch.save(model.state_dict(), '../models/transformer_lstm_wts.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../models/transformer_lstm_wts.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SY\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "label_dict = {0 : 'AE', 1 : 'BH', 2 : 'DZ', 3 : 'EG', 4 : 'IQ', 5 : 'JO', 6 : 'KW', 7 : 'LB', 8 : 'LY',\n",
    "              9 : 'MA', 10 : 'OM', 11 : 'PL', 12 : 'QA', 13 : 'SA', 14 : 'SD', 15 : 'SY', 16 : 'TN', 17 : 'YE'}\n",
    "\n",
    "\n",
    "s = 'ÙŠØ®Ø±Ø¨ Ø¨ÙŠØª Ø¹ÙŠÙˆÙ†Ùƒ ÙŠØ§ ØµÙˆÙÙŠØ§ Ø´Ùˆ Ø­Ù„ÙˆÙŠÙ†'\n",
    "\n",
    "inputs = arabert_tokenizer([s])\n",
    "input_ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n",
    "attention_mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "model(input_ids,attention_mask)\n",
    "\n",
    "print(label_dict[np.argmax(model(input_ids,attention_mask).detach().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
